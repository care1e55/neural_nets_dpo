{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прореживание сети \n",
    "\n",
    "А теперь проерживание сетки! Как это ни странно, код будет очень похож, за исключением одного нюанса. Нужно удалить, а не добавить столбец/строчку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Для визуализации обучения \n",
    "def visualize(l1,l2, h1, h2):\n",
    "    plt.figure(figsize=(20,5)) \n",
    "    epo_range = range(1,len(h1)+1)\n",
    "    tick_range = range(1,len(h1)+1,2)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Learning loss')\n",
    "    plt.plot(epo_range,l1, label='train set')\n",
    "    plt.plot(epo_range,l2, label='valid set')\n",
    "    plt.grid()\n",
    "    plt.xticks(tick_range)\n",
    "    plt.legend(title = 'Loss at:')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Learning accuracy')\n",
    "    plt.plot(epo_range,h1, label='train set')\n",
    "    plt.plot(epo_range,h2, label='valid set')\n",
    "    plt.grid()\n",
    "    plt.xticks(tick_range)\n",
    "    plt.ylim(0, 1.)\n",
    "    plt.legend(title = 'Accuracy at:')\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras  \n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# внутри keras уже есть набор данных, подгрузим его \n",
    "def load_dataset( ):\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # нормализация матриц\n",
    "    X_train = X_train.astype(np.float32) / 255.\n",
    "    X_test = X_test.astype(np.float32) / 255.\n",
    "    \n",
    "    # вытягиваем матрицы [n,28,28] в вектрора [n,28**2]\n",
    "    X_train = X_train.reshape(X_train.shape[0],28**2)\n",
    "    X_test = X_test.reshape(X_test.shape[0],28**2)\n",
    "    \n",
    "    # сделаем OHE для таргета\n",
    "    y_train = keras.utils.to_categorical(y_train).astype(np.float32)\n",
    "    y_test = keras.utils.to_categorical(y_test).astype(np.float32)\n",
    "\n",
    "    # последние 10000 примеров из трэйна оставим для валидации\n",
    "    X_train, X_val, y_train, y_val =  train_test_split(X_train, y_train, test_size=0.15, stratify=y_train)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свой генератор батчей. Как думаете, что с ним не так? Исправьте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(X, y, batch_size):\n",
    "    n_batches = int(X.shape[0]/batch_size) + 1\n",
    "    for batch_idx in range(n_batches):\n",
    "        indices = (batch_idx*batch_size, min(X.shape[0], (batch_idx+1)*batch_size))\n",
    "        yield X[indices[0]:indices[1]], y[indices[0]:indices[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем своего монстра. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_simple():\n",
    "    \n",
    "    def __init__(self, L1, L2, L3, L4, L5, learning_rate):\n",
    "        \n",
    "        self.W1 = tf.Variable(tf.random.truncated_normal([784, L1], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.zeros([L1]))\n",
    "\n",
    "        self.W2 = tf.Variable(tf.random.truncated_normal([L1, L2], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.zeros([L2]))\n",
    "\n",
    "        self.W3 = tf.Variable(tf.random.truncated_normal([L2, L3], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.zeros([L3]))\n",
    "        \n",
    "        self.W4 = tf.Variable(tf.random.truncated_normal([L3, L4], stddev=0.1))\n",
    "        self.b4 = tf.Variable(tf.zeros([L4]))\n",
    "        \n",
    "        self.W5 = tf.Variable(tf.random.truncated_normal([L4, L5], stddev=0.1))\n",
    "        self.b5 = tf.Variable(tf.zeros([L5]))\n",
    "        \n",
    "        self.opt = tf.optimizers.Adam(learning_rate)\n",
    "              \n",
    "    def generate_prediction_logits(self,X_train):\n",
    "        Y1 = tf.nn.relu(tf.matmul(X_train, self.W1) + self.b1)\n",
    "        Y2 = tf.nn.relu(tf.matmul(Y1, self.W2) + self.b2)\n",
    "        Y3 = tf.nn.relu(tf.matmul(Y2, self.W3) + self.b3)\n",
    "        Y4 = tf.nn.relu(tf.matmul(Y3, self.W4) + self.b4)\n",
    "        Ylogits = tf.matmul(Y4, self.W5) + self.b5\n",
    "        return Ylogits\n",
    "    \n",
    "    def make_loss(self,X_train,y_train):\n",
    "        Ylogits = self.generate_prediction_logits(X_train)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=y_train)\n",
    "        return tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    def make_learning_step(self,X_train,y_train):\n",
    "        # находим loss и пробрасываем градиент\n",
    "        with tf.GradientTape() as g:\n",
    "            loss = self.make_loss(X_train, y_train)\n",
    "        gradients = g.gradient(loss, [self.W1,self.W2,self.W3,self.b1, self.b2,\n",
    "                                     self.b3])\n",
    "        self.opt.apply_gradients(zip(gradients, [self.W1,self.W2,self.W3,self.b1, self.b2,\n",
    "                                     self.b3]))\n",
    "        return gradients\n",
    "    \n",
    "    def model_acc(self,x_train,y_train):\n",
    "        Ylogits = self.generate_prediction_logits(x_train)\n",
    "        Y = tf.nn.softmax(Ylogits)\n",
    "        correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_train, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вопрос:__ куда надо добавить декоратор, чтобы училось более бысто?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = 100\n",
    "L2 = 200\n",
    "L3 = 300\n",
    "L4 = 350\n",
    "L5 = 10\n",
    " \n",
    "learning_rate = 0.003\n",
    "Our_Model = model_simple(L1,L2,L3,L4,L5,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 100\n",
    "loss_test, loss_train  = [ ], [ ] \n",
    "acc_test, acc_train = [ ], [ ]\n",
    "\n",
    "\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    \n",
    "    # число батчей\n",
    "    num_batches = (X_train.shape[0] / batch_size) + 1\n",
    "    batch_gen = batches_generator(X_train, y_train, batch_size)\n",
    "    \n",
    "    # пошёл цикл по батчам \n",
    "    for X_batch, y_batch in batch_gen:\n",
    "        Our_Model.make_learning_step(X_batch,y_batch)\n",
    "\n",
    "    # ищем метрики в конце эпохи\n",
    "    loss_train.append(Our_Model.make_loss(X_train,y_train))\n",
    "    loss_test.append(Our_Model.make_loss(X_val,y_val))\n",
    "    \n",
    "    acc_train.append(Our_Model.model_acc(X_train,y_train))\n",
    "    acc_test.append(Our_Model.model_acc(X_val,y_val))\n",
    "\n",
    "    visualize(loss_train, loss_test, acc_train, acc_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть что за веса получилсь можно вот так: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our_Model.W5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our_Model.W5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сохраним получившиеся веса, например в виде numpy массивов, чтобы в случае чего мы могли бы их подгрузить. Модели можно сохранять ещё и как графы вычислений. Но про это мы поговорим позже. \n",
    "\n",
    "Ежу понятно, что копипста строчек для сохранения матриц - не самая хорошая идея. Чтобы не заниматься такой ерундой, нужно унаследовать класс, в котором мы задаём модель, от Model. Но об этом мы поговорим в следующий раз. Сейчас пока что мы акцентируем внимание именно на том, что нейросетка - это просто поток из кучи матриц. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_1_save = Our_Model.W1.numpy()\n",
    "bias_1_save = Our_Model.b1.numpy()\n",
    "np.save('weight_1_save.npy', weight_1_save)\n",
    "np.save('bias_1_save.npy', bias_1_save)\n",
    "\n",
    "weight_2_save = Our_Model.W2.numpy()\n",
    "bias_2_save = Our_Model.W2.numpy()\n",
    "np.save('weight_2_save.npy', weight_2_save)\n",
    "np.save('bias_2_save.npy', bias_2_save)\n",
    "\n",
    "weight_3_save = Our_Model.W3.numpy()\n",
    "bias_3_save = Our_Model.W3.numpy()\n",
    "np.save('weight_3_save.npy', weight_3_save)\n",
    "np.save('bias_3_save.npy', bias_3_save)\n",
    "\n",
    "weight_4_save = Our_Model.W4.numpy()\n",
    "bias_4_save = Our_Model.W4.numpy()\n",
    "np.save('weight_4_save.npy', weight_4_save)\n",
    "np.save('bias_4_save.npy', bias_4_save)\n",
    "\n",
    "weight_5_save = Our_Model.W5.numpy()\n",
    "bias_5_save = Our_Model.W5.numpy()\n",
    "np.save('weight_5_save.npy', weight_5_save)\n",
    "np.save('bias_5_save.npy', bias_5_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузить веса можно следующим способом\n",
    "bias_1_save = np.load('bias_1_save.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что значит на матричном уровне, что нейрон отключен? Как можно проверить, что стало лучше или хуже? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_1 = Our_Model.W1.numpy()\n",
    "bias_1 = Our_Model.b1.numpy() # загрузили веса\n",
    "\n",
    "# посчитаем наш базовый лосс на валидации\n",
    "loss_base = ...  # ВАШ КОД! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем следующую процедуру-отключаем (зануляем) каждый нейрон, смотрим насколько у нас меняется итоговый loss на валидации. Это изменение - \"значимость\" каждого нейрона. Давайте по этой характеристике отранжируем нейроны. Самый незначимый будем выбрасывать из сетки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_in_loss = [] # сохраним лист с изменениями\n",
    "\n",
    "for i in tqdm(range(weight_1.shape[1])):\n",
    "    \n",
    "    weight_1=weight_1_save.copy() \n",
    "    bias_1=bias_1_save.copy() # зачем мы копируем ? \n",
    "    \n",
    "    weight_1[:,i] = 0\n",
    "    bias_1[i] = 0\n",
    "    Our_Model.W1 = weight_1\n",
    "    Our_Model.b1 = bias_1\n",
    "    \n",
    "    # наш новый loss с изменениями\n",
    "    loss_new = Our_Model.make_loss(X_val,y_val).numpy() \n",
    "    change_in_loss.append(loss_new-loss_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({'neural_list':list(range(weight_1.shape[1])),\n",
    "             'change_in_loss':change_in_loss}).sort_values('change_in_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь перепишите нейронную сеть, как она должна выглядеть\n",
    "# с исключенным нейронном и поучите еще пару десятков эпох\n",
    "# Удалить строки из numpy можно таким образом\n",
    "# иницализируем нейроны прошлыми результатами\n",
    "# если они где-то потерялись, можно их загрузить\n",
    "# weight_1=np.delete(weight_1_save,axis=0,obj=24) -удалит 24 строку\n",
    "# weight_1=np.delete(weight_1_save,axis=1,obj=24) -удалит 24 столбец\n",
    "# подсказка-следите за размерностями!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашка\n",
    "\n",
    "Сейчас мы сделали добавление нового нейрона на слой довольно убого. Мы просто-напросто пересоздали класс и добавили в него новые веса. Это такое себе занятие. Хочется, чтобы можно было добавлять новый нейрон на конкретный слой с помощью какого-нибудь специального метода, который был бы задан прямо внутри класса. По аналогии хотелось бы увидеть метод, который удаляет нейрон. \n",
    "\n",
    "* Реализуйте внутри нашего класса метод для добавления/удаления нейронов \n",
    "* Подумайте о том как именно можно было бы выбирать нейрон для удаления из сетки (как можно попытаться вычислить значимость каждого конкретного нейрона и выбрать для удаления наименее значимый).\n",
    "* Подумайте как реализовать подсчёт значимости в виде метода и добавьте его в модель. \n",
    "\n",
    "__Бонусный трэк для самых упоротых:__ \n",
    "\n",
    "* Реализуйте метод, который добавляет в нейронку новый слой\n",
    "* Попробуйте подобрать для MNIST оптимальную архитектуру жадным алгоритмом с пошаговым добавлением/выкидыванием нейронов и слоёв. Самостоятельно придумайте правила перебора и способ останавливать его. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
