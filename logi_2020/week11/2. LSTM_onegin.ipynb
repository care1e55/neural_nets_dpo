{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Порождающая RNN\n",
    "\n",
    "Эта тетрадка основана на 6 главе Николенко. Пришло время обучить на каком-нибудь корпусе текстов порождающую нейронную сетку. Для того, чтобы делать это, нужно переработать тексты в приемлимые для работы датасеты. Идея, находящаяся под капотом очень проста: __давайте будем порождать текст буква за буквой,__ рассматривая его как поток символов. Тогда задача очень легко формализуется. Нужно предсказать следущий символ по текущему, то есть на каждом шаге нужно по накопленной истории решать задачу классификации на десяток классов.\n",
    "\n",
    "Вопрос в том, как для такой модели определить тренировочные данные.\n",
    "\n",
    "* __Первый, самый простой вариант__ - взять текст и разбить его на последовательности фиксированной длинны (окна) и предсказывать следущий по предыдущим. В такой ситуации начало и конец тренировочного примера будут довольно \"внезапными\", и артефакты по краям будут мешать генерировать на выходе хороший текст.\n",
    "* __Второй вариант__ - дробить текст на последовательности разной длины, но не слишком большой. Например, на предложения. Затем мы добавим спецсимволы начала и конца предложения и дополним каждую строчку до ммаксимума спецсимволом, означающим пустоту.\n",
    "* __Третий вариант__ - нарезать текст на последовательности примерно одной длины, но при этом правильным образом инициализировать скрытые состояния сетки. Для этого нужно взять наш длинный-длинный вход и превратить его сначала в достаточно широкий прямоугольник размера $N \\times L$, где $L$ - длина каждого тренировочного примера, а $N$ - число тренировочных примеров в исходной последовательности. Пока что $L$ очень велика. Но мы можем разрезать прямоугольник на более маленькие батчи по вертикали. Получиться, что каждый новый батч - продолжение предыдущего и его нужно обучать не с нуля, а с того скрытого состояния, на котором закончился предыдущий батч. \n",
    "\n",
    "Мы будем использовать второй вариант и просто дробить текст на предложения. \n",
    "\n",
    "## 1. Предобработка выборки\n",
    "\n",
    "Будем строить модельна примери Евгения Онегина. Считаем данные, определим три специальных символа: `START_CHAR` будет подставляться перед началом предложеия, `END_CHAR` после его конца, а `PADDING_CHAR` будет заполнять остаток предложения до максимума длины. Для простоты мы также не будетм различать строчные и заглавные буквы, а просто пропустим каждую строчку через `lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_CHAR = '\\b'\n",
    "END_CHAR = '\\t'\n",
    "PADDING_CHAR = '\\a'\n",
    "\n",
    "chars = set([START_CHAR, '\\n', END_CHAR])\n",
    "\n",
    "with open('onegin.txt') as f:\n",
    "    for line in f:\n",
    "        chars.update(list(line.strip().lower()))\n",
    "    \n",
    "# заведём словари с отображением символов в числа и обратно    \n",
    "char_indices = { c : i for i,c in enumerate(sorted(list(chars))) }\n",
    "char_indices[PADDING_CHAR] = 0\n",
    "indices_to_chars = { i : c for c,i in char_indices.items() }\n",
    "num_chars = len(chars)\n",
    "\n",
    "num_chars # уникальные символы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим векторные представления для символов. Это будет просто OHE представления, в котором каждому символу соответствует вектор с одной единицей, за исключением PADDING_CHAR, который будет представляться просто нулевым вектором. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_one(i, sz):\n",
    "    res = np.zeros(sz)\n",
    "    res[i] = 1\n",
    "    return res\n",
    "\n",
    "char_vectors = {\n",
    "    c : (np.zeros(num_chars) if c == PADDING_CHAR else get_one(v, num_chars))\n",
    "    for c,v in char_indices.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь считаем исходный файл ещё разок. На этот раз разобьём его на предложения. Будем брать только предложения длиннее 10 символов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'не мысля гордый свет забавить,\\nвниманье дружбы возлюбя,\\nхотел бы я тебе представить\\nзалог достойнее тебя,\\nдостойнее души прекрасной,\\nсвятой исполненной мечты,\\nпоэзии живой и ясной,\\nвысоких дум и простоты;\\nно так и быть — рукой пристрастной\\nприми собранье пестрых глав,\\nполусмешных, полупечальных,\\nпростонародных, идеальных,\\nнебрежный плод моих забав,\\nбессониц, легких вдохновений,\\nнезрелых и увядших лет,\\nума холодных наблюдений\\nи сердца горестных замет.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_end_markers = set('.!?')\n",
    "\n",
    "sentences = [ ]\n",
    "current_sentence = ''\n",
    "with open('onegin.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        s = line.strip().lower()\n",
    "        if len(s) > 0:\n",
    "            current_sentence += s + '\\n'\n",
    "        if len(s) == 0 or s[-1] in sentence_end_markers:\n",
    "            current_sentence = current_sentence.strip()\n",
    "            if len(current_sentence) > 10:\n",
    "                sentences.append(current_sentence)\n",
    "            current_sentence = ''\n",
    "            \n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следущий шаг - векторизация. Давайте определим процедуру, которая превращает набор предложений в два тензора: $X$ содержит векторы символов, а $Y$ результат, который нам нужно предсказать. Это на самом деле ровно тот же тензор $X$, только сдвинутый на один вектор влево: во время $t$ мы предсказываем символ, который будет стоять на месте $t+1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(sentences):\n",
    "    max_sentence_len = np.max([len(x) for x in sentences])\n",
    "    X = np.zeros((len(sentences), max_sentence_len, len(chars)), dtype = np.bool)\n",
    "    Y = np.zeros((len(sentences), max_sentence_len, len(chars)), dtype = np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        char_seq = (START_CHAR + sentence + END_CHAR).ljust(\n",
    "                      max_sentence_len + 1, PADDING_CHAR)\n",
    "        for t in range(max_sentence_len):\n",
    "            X[i, t, :] = char_vectors[char_seq[t]]\n",
    "            Y[i, t, :] = char_vectors[char_seq[t+1]]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для экономии памяти и ускорения мы вместо единичек и нулей ставим булевы значеия TRUE и FALSE. Обратите внимание, что мы в начало каждого предложения и в его конец добавили особые символы и дополнили каждое из ни пустотами до длины максимального предложения функцией `ljust`. Наконец мы готовы к строительству нейросетки.\n",
    "\n",
    "## 2. Архитектура\n",
    "\n",
    "Построим простую модель. Добавим один уровень LSTM ячеек, результат работы которых будем пропускать через один полносвязный слой, на котором и будет происходить классификация. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"tanh\", return_sequences=True, input_shape=(None, 75), units=128)`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=75)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, TimeDistributed, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(output_dim = 128, activation = 'tanh', \n",
    "               return_sequences = True, input_dim = num_chars))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(output_dim = num_chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура! Архитектура собралась. Прокомментируем её. \n",
    "\n",
    "* Параметр `output_dim = 128` означает, что наш слой состоит из 128 LSTM-ячеек, на этом слое в качестве функции активации используем тангенс. Можно при желании поменять на сигмоиду... \n",
    "* Параметр `input_dim = X.shape[2]` задаёт размерность входа, то есть число символов. В нашем случае это `len(chars)`.\n",
    "* Параметр `return_sequences = True` здесь самый интересный. По умолчанию LSTM-ячейка идёт по входной последовательности, в нашем случае предложению, и на выход выдаёт только самый последний рещультат. Мы бы хотели, чтобы она сохраняла всю последовательность по мере продвижения.\n",
    "\n",
    "Слой `TimeDistributed` тоже является для нас новой штукой. Его фишка в том, что веса полносвязного словя не меняются во времени. Для всех выходов из LSTM всегда используются одни и те же веса. \n",
    "\n",
    "Вся наша собранная модель нарисована на картинке ниже: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель осталось скомпилировать и начать процедуру оптимизации. Как вы помните, рекурентные нейронки страдают проблемой взрыва градиентов. Чтобы этой проблемы избежать, их купируют (обрезают). Для этого используют два параметра: \n",
    "\n",
    "* `clipnorm` будет масштабировать вектор градиента так, чтобы его норма не превысила заданного порога\n",
    "* `clipvalue` будет просто обрезать до заданного порога каждую компоненту градиента по отдельности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(clipnorm = 1.), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы почти готовы к обучению. Остались две маленькие детали. Когда мы писали функцию `get_matrices`, мы оформили её так, что размерность тензоров на выходе зависит от максимальной длины предложений на входе. Мы сделали так специально, чтобы подавать в нашу нейросетку данные батчами. Нужно обернуть функцию `get_matrices` в генератор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделим тестовую выборку\n",
    "test_indices = np.random.choice(range(len(sentences)), int(len(sentences) * 0.05))\n",
    "\n",
    "sentences_train = [sentences[x] for x in set(range(len(sentences))) - set(test_indices)]\n",
    "sentences_test = [sentences[x] for x in test_indices]\n",
    "\n",
    "sentences_train = sorted(sentences_train, key = lambda x : len(x))\n",
    "X_test, y_test = get_matrices(sentences_test)\n",
    "\n",
    "batch_size = 16\n",
    "def generate_batch():\n",
    "    while True:\n",
    "        for i in range( int(len(sentences_train)/batch_size)):\n",
    "            sentences_batch = sentences_train[i*batch_size : (i+1)*batch_size]\n",
    "            yield get_matrices(sentences_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь функция `generate_batch` последовательно будет проходить наш датасет, разбивая его на мини-батчи по 16 предложений. А чтобы это имело ещё больше смысла мы предварительно отсортировали `sentences_train` по длине, так что теперь мини-батчи будут иметь последовательно увеличивающуюся ширину, предложения в них будут иметь примерно одинаковую длину и заполнять `PADDING_CHAR` придется не так много остатков. \n",
    "\n",
    "В ходе обучения модели нам часто хочется наблюдать за какими-нибудь её особенностями. В этом обычно помогают специальные функции: функции обратного вызова (callbacks). Если в этот метод передать список функций, то они будут запускаться после каждой эпохи обучения. И мы сможем отслеживать интересующую нас при обучении сетки статистику. Давайте добавим две стандартные функции обратного вызова и напишем одну свою. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу же добавляем свою. Как вы могли бы в с помнить, на самой первой паре мы говорили о Softmax. Мы говорили, что это мягкий максимум. Он при трансформации полученных для каждой категории чисел в вероятности, пытается приподнять самое большое число и приопустить маленькие, мягко выделяя максимум. \n",
    "\n",
    "Силу такого выделения можно контролировать. Для этого в формулу вшивается дополнительный параметр $T$, который называют температурой сэмплирования (см пример на странице 267 Николенко). \n",
    "\n",
    "$$ p(w) \\propto e^{-\\frac{1}{T} x_{w}} $$\n",
    "\n",
    "Если взять $T$ очень большим, то показатели экспоненты окажутся небольшими по модулю и возведения в степень будут близки. На выходе получится близкое к равномерному распределение. Сэмплирование будет довольно случайным. Взвинчивание $T$ позволяет делать сэмплирование конкретным. При $T \\to 0$ распределение будет вырожденным. \n",
    "\n",
    "Напишем колбэк, который будет подставлять в сетку разные значения температуры при генерации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class CharSampler(Callback):\n",
    "    def __init__(self, char_vectors, model):\n",
    "        self.char_vectors = char_vectors\n",
    "        self.model = model \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch = 0\n",
    "        if os.path.isfile('output_file'):\n",
    "            os.remove('output_file')\n",
    "            \n",
    "    def sample(self, preds, temperature=1.0):\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds)/temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "    \n",
    "    def sample_one(self, T):\n",
    "        result = START_CHAR\n",
    "        while len(result) < 500:\n",
    "            Xsampled = np.zeros((1, len(result), num_chars))\n",
    "            for t,c in enumerate(list(result)):\n",
    "                Xsampled[0, t, :] = self.char_vectors[c]\n",
    "            ysampled = self.model.predict(Xsampled, batch_size=1)[0,:]\n",
    "            yv = ysampled[len(result) - 1, :]\n",
    "            selected_char = indices_to_chars[self.sample(yv, T)]\n",
    "            if selected_char == END_CHAR:\n",
    "                break\n",
    "            result = result + selected_char\n",
    "        return result\n",
    "\n",
    "    def on_epoch_end(self, batch, logs = {}):\n",
    "        self.epoch_end = self.epoch + 1\n",
    "        if self.epoch % 50 == 0:\n",
    "            print(\"\\nEpoch %d text sampling:\" % self.epoch)\n",
    "            with open('output_file', 'a') as outf:\n",
    "                outf.write('\\n======= Epoch %d =======\\n' % self.epoch)\n",
    "                for T in [0.3, 0.5, 0.7, 0.9, 1.1]:\n",
    "                    print('\\tsampling, T = %.1f...' % T)\n",
    "                    for _ in range(5):\n",
    "                        self.model.reset_states()\n",
    "                        res = self.sample_one(T)\n",
    "                        outf.write('\\nT = %.1f\\n%s\\n' % (T, res[1:]))\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в сетку ещё один колбэк для логирования результатов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "cb_logger = CSVLogger('sim/' + 'onegin' + '.log')\n",
    "cb_sampler = CharSampler(char_vectors, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец мы готовы учить. Для обучения по батчам испольщуем `fit_generate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., 1136, verbose=True, validation_data=(array([[[..., callbacks=[<keras.ca..., epochs=1000)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1136/1136 [==============================] - 97s 85ms/step - loss: 2.7313 - accuracy: 0.2187 - val_loss: 0.7570 - val_accuracy: 0.0855\n",
      "\n",
      "Epoch 0 text sampling:\n",
      "\tsampling, T = 0.3...\n",
      "\tsampling, T = 0.5...\n",
      "\tsampling, T = 0.7...\n",
      "\tsampling, T = 0.9...\n",
      "\tsampling, T = 1.1...\n",
      "Epoch 2/1000\n",
      "1136/1136 [==============================] - 93s 82ms/step - loss: 2.3568 - accuracy: 0.2814 - val_loss: 0.7217 - val_accuracy: 0.0943\n",
      "\n",
      "Epoch 0 text sampling:\n",
      "\tsampling, T = 0.3...\n",
      "\tsampling, T = 0.5...\n",
      "\tsampling, T = 0.7...\n",
      "\tsampling, T = 0.9...\n",
      "\tsampling, T = 1.1...\n",
      "Epoch 3/1000\n",
      "1136/1136 [==============================] - 91s 81ms/step - loss: 2.2188 - accuracy: 0.3135 - val_loss: 0.6973 - val_accuracy: 0.1018\n",
      "\n",
      "Epoch 0 text sampling:\n",
      "\tsampling, T = 0.3...\n",
      "\tsampling, T = 0.5...\n",
      "\tsampling, T = 0.7...\n",
      "\tsampling, T = 0.9...\n",
      "\tsampling, T = 1.1...\n",
      "Epoch 4/1000\n",
      "  68/1136 [>.............................] - ETA: 1:20 - loss: 2.1538 - accuracy: 0.3306"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4771055e80db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                      callbacks=[cb_logger, cb_sampler] )\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generate_batch(),\n",
    "                     int(len(sentences_train) / batch_size) * batch_size,\n",
    "                     nb_epoch=1000,\n",
    "                     verbose=True, \n",
    "                     validation_data = (X_test, y_test), \n",
    "                     callbacks=[cb_logger, cb_sampler] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
