{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "[3](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)\n",
    "[4](http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да простит меня автор задания, но я буду решать эту задачку через диференциалы и писать буду на русском. Да не взайдёт на голову мне кара за сию вольность. Итак, $F(x) = x^Tx$.\n",
    "\n",
    "$$\n",
    "dF(x) = d(x^Tx) = dx^Tx + x^T dx = x^Tdx + x^Tdx = 2 x^Tdx.\n",
    "$$ \n",
    "\n",
    "В третьем равенстве мы воспользовались тем, что $dx^Tx$ константа и протранспонировали её. В итоге получается, что \n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 2x^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии с тем, что было выше $F(A) = tr(AB)$. Функция бьёт из матриц в скаляр. Дифференциал выглядит как $d F = J^T dA$, где $J^T$ необходимая нам производная. \n",
    "\n",
    "$$\n",
    "dF = d(tr(AB)) = tr(dAB) = tr(BdA) \n",
    "$$\n",
    "\n",
    "Выходит, что \n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} = B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас $F(x) = x^TAc$. Получается, что \n",
    "\n",
    "$$\n",
    "dF = dx^TAc = c^TA^Tdx = (Ac)^Tdx\n",
    "$$\n",
    "\n",
    "Снова воспльзовались тем, что транспонированная константа снова будет константой. Выходит, что \n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} =(AC)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас $F(A) = x^TAc$. Функция бьёт из множества матриц в скаляры. Производная выглядит также как во втором задании.\n",
    "\n",
    "$$\n",
    "dF = dx^TAc = x^TdAc = tr(x^TdAc) = tr(cx^TdA).\n",
    "$$\n",
    "\n",
    "То есть \n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} = xc^T\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
    "$$\n",
    "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dS} = ? \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First approach\n",
    "Using ex.2 and the fact:\n",
    "$$\n",
    "|| X ||_F^2 = tr(XX^T) \n",
    "$$ \n",
    "it is easy to derive gradients (you can find it in one of the refs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте искать!\n",
    "\n",
    "У нас $J(S) = tr((X - AS)(X - AS)^T)$. \n",
    "\n",
    "\\begin{multline*}\n",
    "dJ = tr[d(X-AS)(X-AS)^T + (X - AS) d(X - AS)^T] = tr[-A dS (X - AS)^T  - (X - AS) dS^T A^T] = \\\\ = tr[-(X-AS)^TAdS - dS^TA^T(X-AS)] = tr[-(X-AS)^TAdS - (X-AS)^T A ds] = tr[-2(X-AS)^TAdS]\n",
    "\\end{multline*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По ходу решения мы переодически разносили $tr(A + B) = tr(A) + tr(B)$ и под знаками следа передвигали матрицы и транспонировали их. В итоге получаем, что \n",
    "\n",
    "$$\n",
    "\\frac{dА}{dS} = -2A^T(X - AS)\n",
    "$$ \n",
    "\n",
    "Если приравнять её к нулю и решить уравнение получим, что $\\hat S = (A^TA)^{-1}A^TX$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Second approach\n",
    "You can use *slightly different techniques* if they suits you. Take a look at this derivation:\n",
    "<img src=\"grad.png\">\n",
    "(excerpt from [Handbook of blind source separation, Jutten, page 517](https://books.google.ru/books?id=PTbj03bYH6kC&printsec=frontcover&dq=Handbook+of+Blind+Source+Separation&hl=en&sa=X&ved=0ahUKEwi-q_apiJDLAhULvXIKHVXJDWcQ6AEIHDAA#v=onepage&q=Handbook%20of%20Blind%20Source%20Separation&f=false), open for better picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Third approach\n",
    "And finally we can use chain rule! **YOUR TURN** to do it.\n",
    "let $ F = AS $ \n",
    "\n",
    "**Find**\n",
    "$$\n",
    "\\frac{dJ}{dF} =  2F = 2(X - AS)\n",
    "$$ \n",
    "and \n",
    "$$\n",
    "\\frac{dF}{dS} =  - A^T\n",
    "$$ \n",
    "(the shape should be $ NM \\times RM )$.\n",
    "\n",
    "Now it is easy do get desired gradients:\n",
    "$$\n",
    "\\frac{dJ}{dS} =  -2 A^T (X-AS)\n",
    "$$ \n",
    "\n",
    "Тут мы использовали для поиска второй производной тот факт, что функция бьёт из матриц в скаляры как и во втором упражнении. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
