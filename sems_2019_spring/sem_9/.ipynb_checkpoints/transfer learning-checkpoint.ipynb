{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning \n",
    "\n",
    "Transfer learning это когда ты берёшь чужую модель и адаптируешь её под свою задачу. В этой тетрадке будет два примера transfer learning. Для эмбедингов и для картинок. \n",
    "\n",
    "\n",
    "##  Про tensorhub\n",
    "\n",
    "Каждый раз, обучая нейронку, мы сначала рандомно инициализируем веса, а после в ходе бэкпропа обучаем модель. Если мы сразу же угадываем хорошие веса, модель сходится быстрее. Иногда можно брать в качестве инициализации веса, полученные другими исследователями и на их основе дообучать модель под свой выход. Это здорово упрощает задачу обучения и экономит многие часы работы. Для такого дележа в $2017$ году была придумана платформа __TensorFlow Hub.__ \n",
    "\n",
    "Сегодня с помощью этой платформы разработчики делятся друг с другом уже готовыми предобученными весами. Для работы библиотеки нужна версия tensorflow выше `1.7`. Инструкцию по установке можно найти на [сайте tensorflow.](https://www.tensorflow.org/hub/installation)\n",
    "\n",
    "В случае проблем с установкой, повысить версию tensorflow до актуальной помогает команда:\n",
    "\n",
    "```python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ELMO-эмбединги.\n",
    "\n",
    "\n",
    "[ ](https://simkl.in/fanart/17/17052285f7e1391f7_0.jpg)\n",
    "\n",
    "Пробуем воспользоваться хабом. Скачаем [модель от IPavlov с эмбедингами для русскоязычных новостей.](http://docs.deeppavlov.ai/en/master/intro/pretrained_vectors.html) Примеры использования хаба с моделями IPavlov можно найти [в документации.](http://docs.deeppavlov.ai/en/latest/apiref/models/embedders.html#deeppavlov.models.embedders.elmo_embedder.ELMoEmbedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/r2/6lthpk110g7d1kjwlgx6p7100000gn/T/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz'.\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 30.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 60.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 90.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 120.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 150.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 180.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 210.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 240.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 270.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 300.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 330.02MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 511.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 531.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 561.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 591.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 621.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 651.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 681.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 711.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 741.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 771.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 801.06MB\n",
      "INFO:tensorflow:Downloading http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz: 831.06MB\n",
      "INFO:tensorflow:Downloaded http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz, Total size: 858.71MB\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz'.\n"
     ]
    }
   ],
   "source": [
    "# подгружаем модель\n",
    "elmo = hub.Module(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем сессию в tensorflow и применяем к чему-нибудь предобученную нейросетку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.05817385,  0.22493355, -0.1920293 , ..., -0.14448947,\n",
       "         -0.1242556 ,  1.0148408 ],\n",
       "        [ 0.5359629 ,  0.28685376,  0.28028587, ..., -0.08028417,\n",
       "          0.4908908 ,  0.75939935]],\n",
       "\n",
       "       [[ 0.34336394,  1.0031183 , -0.15972564, ...,  1.2442503 ,\n",
       "          0.6102935 ,  0.43388352],\n",
       "        [ 0.05370751,  0.02260921,  0.01074906, ...,  0.08748816,\n",
       "         -0.0066415 , -0.01344293]]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "embeddings = elmo([\"это предложение\", \"word\"], signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также hub поддерживает токенизированный формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.60400033, -0.16130012,  0.5647884 , ..., -0.00376102,\n",
       "         -0.0382006 ,  0.26321295],\n",
       "        [ 0.01834123,  0.17055827,  0.5311495 , ..., -0.56755346,\n",
       "          0.62669814, -0.05939047],\n",
       "        [ 0.32425952,  0.17909637,  0.01657113, ...,  0.1866094 ,\n",
       "          0.7392498 ,  0.08285775]],\n",
       "\n",
       "       [[ 1.1322286 ,  0.19077665, -0.17811388, ...,  0.42973173,\n",
       "          0.23391487, -0.01294377],\n",
       "        [ 0.05370751,  0.02260921,  0.01074906, ...,  0.08748816,\n",
       "         -0.0066415 , -0.01344293],\n",
       "        [ 0.05370751,  0.02260921,  0.01074906, ...,  0.08748816,\n",
       "         -0.0066415 , -0.01344293]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_input = [[\"мама\", \"мыла\", \"раму\"], [\"рама\", \"\", \"\"]]\n",
    "tokens_length = [3, 1]\n",
    "embeddings = elmo(\n",
    "    inputs={\n",
    "            \"tokens\": tokens_input,\n",
    "            \"sequence_len\": tokens_length\n",
    "            },\n",
    "    signature=\"tokens\",\n",
    "    as_dict=True)[\"elmo\"]\n",
    "\n",
    "sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(1024)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме новостей в [документации проекта можно найти](http://docs.deeppavlov.ai/en/master/intro/pretrained_vectors.html) википедию, русскоязычный твиттер и многие другие эмбединги. Там же можно найти довольно много полноценных классификаторов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Впихиваем в свою нейронку предобученный embedding\n",
    "\n",
    "Аналогично можно делать с абсолютно любыми нейросетками и слоями. Например, давайте попробуем имплиментировать предобученный Google ELMO embeddig слой внутрь сетки. В качестве примера возьмём корпус из imdb отзывов о фильмах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Про опции подгрузки выборки подробнее в документации: \n",
    "# https://keras.io/datasets/\n",
    "NUM_WORDS = 25000 # использовать только 25000 самых встречаемых слов\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n",
    "\n",
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k : v for k,v in word_to_id.items()}\n",
    "\n",
    "id_to_word = {value : key for key,value in word_to_id.items()}\n",
    "\n",
    "X_train = [' '.join([id_to_word[i] for i in item]) for item in X_train]\n",
    "X_test = [' '.join([id_to_word[i] for i in item]) for item in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room titillate it so heart shows to years of every never going villaronga help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пример отзыва\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# какой отзыв\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если я правильно понял [инфомрацию с github по tensofglow hub,](https://github.com/tensorflow/hub/issues/155) также просто как для картинок, embedding слой внутрь сетки завернуть не получится. Однако на просторах интернета [можно найти](https://towardsdatascience.com/elmo-embeddings-in-keras-with-tensorflow-hub-7eb6f0145440)  готовые классы для имплиментации хаба их эмбеддингов в сетку. Если сможете сделать это проще, расскажите мне как."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import Layer\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['default']\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return K.not_equal(inputs, '--PAD--')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем с этим, вручную собранным слоем, нейросетку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
    "    embedding = ElmoEmbeddingLayer()(input_text)\n",
    "    dense = layers.Dense(256, activation='relu')(embedding)\n",
    "    pred = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=[input_text], outputs=pred)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "elmo_embedding_layer_2 (Elmo (None, 1024)              4         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 262,661\n",
      "Trainable params: 262,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем одну долгую эпоху. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=1,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы выставили для эмбедингов параметр так, что слой тренируемый. Это не очень хорошая идея. Обучение будет идти очень долго. Конечно же к обучению можно прикрутить любые колбэки, построить любые картинки и многое другое. Ещё можно сохранить модель, а также посмотреть на её качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ElmoModel.h5') # Сохранение модели\n",
    "pre_save_preds = model.predict(X_text[0:100]) # Прогнозы модель\n",
    "\n",
    "# Подгрузка модели и её стройка\n",
    "model = None\n",
    "model = build_model()\n",
    "model.load_weights('ElmoModel.h5')\n",
    "\n",
    "post_save_preds = model.predict(test_text[0:100]) # прогнозы после подгрузки\n",
    "all(pre_save_preds == post_save_preds) # Конечно же они совпадают :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
