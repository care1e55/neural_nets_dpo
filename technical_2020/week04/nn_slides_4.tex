%!TEX TS-program = xelatex
\documentclass[notes,12pt, aspectratio=169]{beamer}

\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools}  % пакеты для математики
\usepackage{minted}

\usepackage[english, russian]{babel} % выбор языка для документа
\usepackage[utf8]{inputenc} % задание utf8 кодировки исходного tex файла
\usepackage[X2,T2A]{fontenc}        % кодировка

\usepackage{fontspec}         % пакет для подгрузки шрифтов
\setmainfont{Helvetica}  % задаёт основной шрифт документа

% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
\newfontfamily{\cyrillicfonttt}{Helvetica}
\newfontfamily{\cyrillicfont}{Helvetica}
\newfontfamily{\cyrillicfontsf}{Helvetica}

\usepackage{unicode-math}     % пакет для установки математического шрифта
% \setmathfont{Neo Euler} % шрифт для математики

\usepackage{polyglossia}      % Пакет, который позволяет подгружать русские буквы
\setdefaultlanguage{russian}  % Основной язык документа
\setotherlanguage{english}    % Второстепенный язык документа

% Шрифт для кода
\setmonofont[Scale=0.85]{Monaco}
\usepackage{verbments}

\usepackage{pgfpages}
% These slides also contain speaker notes. You can print just the slides,
% just the notes, or both, depending on the setting below. Comment out the want
% you want.
%\setbeameroption{hide notes} % Only slide
%\setbeameroption{show only notes} % Only notes
%\setbeameroption{show notes on second screen=right} % Both

\usepackage{array}

\usepackage{tikz}
\usepackage{verbatim}
\setbeamertemplate{note page}{\pagecolor{yellow!5}\insertnote}
\usetikzlibrary{positioning}
\usetikzlibrary{snakes}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{shapes.misc}
\usetikzlibrary{matrix,shapes,arrows,fit,tikzmark}

\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{multimedia}
\usepackage{multirow}
\usepackage{dcolumn}
\usepackage{bbm}
\newcolumntype{d}[0]{D{.}{.}{5}}

\usepackage{changepage}
\usepackage{appendixnumberbeamer}
\newcommand{\beginbackup}{
   \newcounter{framenumbervorappendix}
   \setcounter{framenumbervorappendix}{\value{framenumber}}
   \setbeamertemplate{footline}
   {
     \leavevmode%
     \hline
     box{%
       \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,right]{footlinecolor}%
%         \insertframenumber  \hspace*{2ex} 
       \end{beamercolorbox}}%
     \vskip0pt%
   }
 }
\newcommand{\backupend}{
   \addtocounter{framenumbervorappendix}{-\value{framenumber}}
   \addtocounter{framenumber}{\value{framenumbervorappendix}} 
}

% для имитации питоновского синтаксиса 
\newcommand{\pgr}[1]{{\color{green} \textbf{#1}}}


%%%%%%%%%% Работа с картинками %%%%%%%%%
\usepackage{graphicx}                  % Для вставки рисунков
\usepackage{graphics}
\graphicspath{{images/}}    % можно указать папки с картинками
\usepackage{wrapfig}                   % Обтекание рисунков и таблиц текстом

\usepackage[space]{grffile}
\usepackage{booktabs}

% These are my colors -- there are many like them, but these ones are mine.
\definecolor{blue}{RGB}{0,114,178}
\definecolor{red}{RGB}{213,94,0}
\definecolor{yellow}{RGB}{240,228,66}
\definecolor{green}{RGB}{0,128, 0}

\hypersetup{
  colorlinks=false,
  linkbordercolor = {white},
  linkcolor = {blue}
}


%% I use a beige off white for my background
\definecolor{MyBackground}{RGB}{255,253,218}

%% Uncomment this if you want to change the background color to something else
%\setbeamercolor{background canvas}{bg=MyBackground}

%% Change the bg color to adjust your transition slide background color!
\newenvironment{transitionframe}{
  \setbeamercolor{background canvas}{bg=yellow}
  \begin{frame}}{
    \end{frame}
}

\setbeamercolor{frametitle}{fg=blue}
\setbeamercolor{title}{fg=black}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{itemize items}{-}
\setbeamercolor{itemize item}{fg=blue}
\setbeamercolor{itemize subitem}{fg=blue}
\setbeamercolor{enumerate item}{fg=blue}
\setbeamercolor{enumerate subitem}{fg=blue}
\setbeamercolor{button}{bg=MyBackground,fg=blue,}


% If you like road maps, rather than having clutter at the top, have a roadmap show up at the end of each section 
% (and after your introduction)
% Uncomment this is if you want the roadmap!
% \AtBeginSection[]
% {
%    \begin{frame}
%        \frametitle{Roadmap of Talk}
%        \tableofcontents[currentsection]
%    \end{frame}
% }
\setbeamercolor{section in toc}{fg=blue}
\setbeamercolor{subsection in toc}{fg=red}
\setbeamersize{text margin left=1em,text margin right=1em} 

% списки, которые растягиваются на всю величину слайда 
\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}

\title[]{\textcolor{blue}{Нейронки для экономистов и вообще}}
\author{Ульянкин Филипп}
\date{\today}

\usepackage{ulem}

\begin{document}

%%% TIKZ STUFF
\tikzset{   
        every picture/.style={remember picture,baseline},
        every node/.style={anchor=base,align=center,outer sep=1.5pt},
        every path/.style={thick},
        }
\newcommand\marktopleft[1]{%
    \tikz[overlay,remember picture] 
        \node (marker-#1-a) at (-.3em,.3em) {};%
}
\newcommand\markbottomright[2]{%
    \tikz[overlay,remember picture] 
        \node (marker-#1-b) at (0em,0em) {};%
}
\tikzstyle{every picture}+=[remember picture] 
\tikzstyle{mybox} =[draw=black, very thick, rectangle, inner sep=10pt, inner ysep=20pt]
\tikzstyle{fancytitle} =[draw=black,fill=red, text=white]
%%%% END TIKZ STUFF

% Title Slide


\begin{frame}
\maketitle
\centering \textbf{\color{blue} Посиделка 4:}  эвристики для обучения сеток
\end{frame}


\begin{frame}{Agenda}
\begin{wideitemize}
	\item  Какими бывают функции активации 
	\item  Инициализация весов в нейросетках 
	\item  Нормализация по батчам 
	\item  Dopout 
	\item  Другие эвристики, используемые при обучении нейронных сетей 
\end{wideitemize} 
\end{frame}


\begin{transitionframe}
	\begin{center}
		\Huge  Какими бывают функции активации и как через них пробросить градиент
	\end{center}
\end{transitionframe}

\begin{frame}{Sigmoid activation}
\begin{center}
	\includegraphics[width=.9\linewidth]{sigmoid_activation_1.png}
\end{center}
\end{frame}

\begin{frame}{Sigmoid activation}
\begin{center}
\includegraphics[width=.8\linewidth]{sigmoid_activation_2.png}
\end{center}
\end{frame}


\begin{frame}{Паралич сети}
	\begin{wideitemize}
		\item  В случае сигмоиды $\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$ 
		
		\item Сигмоида принимает значения на отрезке $[0; 1]$, значит максимальное значение её производной это $\frac{1}{4}$
		
		\item Если сеть очень глубокая, происходит \alert{затухание градиента} 
		
		\item Градиент затухает экспоненциально $\Rightarrow$ сходимость замедляется, более ранние веса обновляются дольше, более глубокие веса быстрее  $\Rightarrow$ значение градиента становится ещё меньше $\Rightarrow$ наступает \alert{паралич сети} 
		
		\item В сетях с небольшим числом слоёв этот эффект незаметен
	\end{wideitemize} 
\end{frame}


\begin{frame}{Центрирование}
\begin{wideitemize}
	\item  Сигмоида не центрирована относительно нуля 
	
	\item  Выход слоя мы обычно находим как $o_i = \sigma(h_i)$, он всегда положительный, значит градиент по весам, идущим на вход в текущий нейрон тоже положительные $\Rightarrow$ они обновляются в одинаковом направлении 
	
	\item Сходимость идёт медленнее и зигзагообразно, но идёт 
	
\end{wideitemize} 
\end{frame}



\begin{frame}{Sigmoid activation}
\begin{center}
\includegraphics[width=.65\linewidth]{sigmoid_activation_2.png}
\end{center}

\begin{itemize}
{\color{red} 
\item Cпособствует затухание градиента

\item Не центрирована относительно нуля

\item Вычислять $e^x$ дорого
}
\end{itemize}
\end{frame}


\begin{frame}{Tanh activation}
\begin{center}
\includegraphics[width=.7\linewidth]{tanh_activation.png}
\end{center}

\begin{itemize}
\item  {\color{green}  Центрирован относительно нуля }

\item  {\color{red}  Всё ещё похож на сигмоиду }

\item $f'(x) = 1 - f(x)^2$  $\Rightarrow$ затухание градиента

\end{itemize}
\end{frame}



\begin{frame}{ReLU activation}
\begin{center}
\includegraphics[width=.8\linewidth]{relu_activation.png}
\end{center}

\begin{itemize}
{ \color{green} 
\item  Быстро вычисляется 

\item  Градиент не затухает

\item  Сходимость сеток ускоряется 
} 
\end{itemize}
\end{frame}


\begin{frame}{ReLU activation}
\begin{center}
\includegraphics[width=.8\linewidth]{relu_activation.png}
\end{center}

\begin{itemize}
{ \color{red} 
\item  Сетка может умереть, если активация занулится на всех нейронах

\item  Не центрирован относительно нуля
} 
\end{itemize}
\end{frame}


\begin{frame}{Зануление ReLU}
\begin{center}
	\includegraphics[width=.7\linewidth]{relu_activation.png}
\end{center}

\begin{wideitemize}
	\item   $f(x) = \max(0, w_0 + w_1 \cdot h_1 + \ldots + w_k \cdot h_k)$
	
	\item  Если $w_0$ инициализировано большим отрицательным числом, нейрон сразу умирает $\Rightarrow$ надо аккуратно инициализировать веса
\end{wideitemize}
\end{frame}


\begin{frame}{Leaky ReLU activation}
\begin{center}
\includegraphics[width=.7\linewidth]{leaky_relu_activation.png}
\end{center}

\begin{itemize}
{ \color{green} 
\item Как ReLU, но не умирает, всё ещё легко считается
\item Производная может быть любого знака
\item Важно, чтобы $a \ne 1$, иначе линейность
} 
\end{itemize}
\end{frame}


\begin{frame}{Что же выбрать}
\begin{wideitemize}
	\item Обычно начинают с $ReLU$, если сетка умирает, берут $Leaky ReLU$
	\item $ReLU$ — стандартный выбор для свёрточных сетей 
	\item В рекурентных сетках чаще всего предпочитается $tanh$ 
	\item На самом деле это не очень важно, нужно держать в голове свойства функций, о которых выше шла речь и понимать, что от перебора функций обычно выигрыш в качестве довольно низкий 
	\item Но есть и исключения ...
\end{wideitemize}

\vfill %
\footnotesize
Краткий обзор функций активаций: {\color{blue}  \url{https://arxiv.org/pdf/1804.02763.pdf}}
\end{frame}


\begin{frame}{ELU activation}
\begin{center}
	\includegraphics[width=.4\linewidth]{elu.png}
\end{center}

\begin{itemize}
	\item ELU улучшает сходимость для глубоких сеток 
	
	$$
	f(x) = \begin{cases} x,  x \ge 0 \\ \alpha \cdot (e^x - 1),  x < 0 \end{cases}
	$$
	
\end{itemize}


\vfill %
\footnotesize
 {\color{blue} \url{https://arxiv.org/pdf/1511.07289.pdf}}
\end{frame}



\begin{transitionframe}
	\begin{center}
		\Huge  Инициализация весов
	\end{center}
\end{transitionframe}


\begin{frame}{Инициализация весов}
\begin{center}
	\includegraphics[width=.6\linewidth]{init1.png}
\end{center}
\begin{wideitemize}
	\item  Что будет, если инициализировать веса нулями?  \pause 
	\item  ${\color{green} \sigma_1 }$ и ${\color{red} \sigma_2 }$  будут обновляться одинаково
\end{wideitemize}
\end{frame}


\begin{frame}{Инициализация весов}
\begin{center}
	\includegraphics[width=.6\linewidth]{init1.png}
\end{center}
\begin{wideitemize}
	\item  Хочется уничтожить симметрию
	\item  Обычно инициализируют маленькими рандомными числами из какого-то распределения (нормальное, равномерное)
\end{wideitemize}
\end{frame}


\begin{frame}{Симметричный случай}
\begin{center}
	\includegraphics[width=.9\linewidth]{init_sim.png}
\end{center}
\end{frame}


\begin{frame}{Инициализация весов}
	\begin{wideitemize}
		\item Наши признаки $X$ пришли к нам из какого-то распределения 
		\item Выход слоя $f(XW)$ будет принадлежать другому распределению 
		\item Если инициализировать веса неправильно, дисперсия распределения може от слоя к слою затухать (сигнал будет теряться) либо наоброт, возрастать (сигнал будет рассеиваться)
		\item Эмпирически было выяснено, что это может портить сходимость для глубоких сеток
		\item  \alert{Хочется контролировать дисперсию} 
	\end{wideitemize}
\end{frame}


\begin{frame}{Инициализация весов}
\begin{wideitemize}
\item  Посмотрим на выход нейрона перед активацией:
	\begin{equation*}
	\begin{aligned} 
	h_i =  w_0 + \sum_{i=1}^{n_{in}} w_i x_i 
	\end{aligned}
	\end{equation*}
	
\item  Дисперсия $h_i$ выражается через дисперсии $x$ и $w$ 
\item  Она не зависит от константы $w_0$ 
\item Будем считать, что веса $w_1, \ldots, w_k \sim iid$,  наблюдения $x_1, \ldots, x_n \sim iid$, а ещё $x_i$ и $w_i$ независимы между собой
\end{wideitemize}
\end{frame}


\begin{frame}{Инициализация весов (симметричный случай)}
	\begin{multline*} 	
		\Var(h_i) = \Var(\sum_{i=1}^{n_{in}} w_i x_i) =  \sum_{i=1}^{n_{in}} \Var(w_i x_i) = \\ 
		= \sum_{i=1}^{n_{in}} [\E(x_i)]^2 \cdot \Var(w_i) + [\E(w_i)]^2 \cdot \Var(x_i)  + \Var(x_i) \cdot \Var(w_i)] = \only<3->{\\  =  \sum_{i=1}^{n_{in}}  \Var(x_i) \cdot \Var(w_i)  } \only<4>{ =  \Var(x)  \color{green} \cdot [n_{in} \cdot \Var(w)]} \only<5->{ =  \Var(x)  \color{green} \cdot \underbrace{[n_{in} \cdot \Var(w)]}_{= 1}}
	\end{multline*}
	
\begin{wideitemize}
	\only<2-3>{
	\item Если функция активации симметричная, тогда $E(x_i) = 0$. Будем инициализировать веса с нулевым средним, тогда $E(w_i) = 0$.}
	\only<4>{ 
	\item  Хотим, чтобы зелёная штука была равна единице, тогда у потока будет всегда постоянная дисперсия, совпадающая с $\Var(x)$.}
\end{wideitemize}
\end{frame}


\begin{frame}{Плохая инициализация весов}
	Пущай 
	
	$$ w_i \sim U \left[ - \frac{1}{\sqrt{n_{in}}};  \frac{1}{\sqrt{n_{in}}}  \right],$$

	тогда 
	
	$$
	\Var(w_i) = \frac{1}{12} \cdot \left( \frac{1}{\sqrt{n_{in}}} + \frac{1}{\sqrt{n_{in}}} \right)^2 = \frac{1}{3 n_{in}} \Rightarrow Var(h_i) = \frac{1}{3} 
	$$

	\alert{Получаем затухание!}
\end{frame}


\begin{frame}{Немного лучше}
	Пущай 
	
	$$ w_i \sim U \left[ - \frac{\sqrt{6}}{\sqrt{n_{in}}};  \frac{\sqrt{6}}{\sqrt{n_{in}}}  \right],$$
	
	тогда 
	
	$$
	\Var(w_i) = \frac{1}{12} \cdot \left( \frac{\sqrt{6}}{\sqrt{n_{in}}} + \frac{\sqrt{6}}{\sqrt{n_{in}}} \right)^2 = \frac{1}{n_{in}} \Rightarrow Var(h_i) = 1 \pause 
	$$
	
	\vfill 
	При forward pass на вход идёт $n_{in}$ набобдений, при backward pass на вход идёт $n_{out}$ градиентов $\Rightarrow$   \alert{канал с дисперсией может быть непостоянным, если число весов от слоя к слою сильно колеблется}
\end{frame}


\begin{frame}{Инициализация Ксавье (Глорота)}
Для неодинаковых размеров слоёв невозможно удволетворить обоим условиям, поэтому обычно усредняют: 

	$$ w_i \sim U \left[ - \frac{\sqrt{6}}{\sqrt{n_{out} + n_{in}}};  \frac{\sqrt{6}}{\sqrt{n_{out} + n_{in}}}  \right],$$

\vfill 

Такая инициализация называется \alert{инициализацией Ксавие (или Глоро)}

\vfill 

По аналогии можно найти формулу для дисперсии нормального распределения, но это уже семинарская задачка :)

\vfill %
\footnotesize
{\color{blue} \url{http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}}
\end{frame}


\begin{frame}{Несимметричный случай}
\begin{center}
	\includegraphics[width=.9\linewidth]{init_relu.png}
\end{center}
\end{frame}


\begin{frame}{Инициализация Хе}
	\begin{multline*} 	
	\Var(h_i) = \Var(\sum_{i=1}^{n_{in}} w_i x_i) =  \sum_{i=1}^{n_{in}} \Var(w_i x_i) = \\ 
	= \sum_{i=1}^{n_{in}} [\E(x_i)]^2 \cdot \Var(w_i) + [\E(w_i)]^2 \cdot \Var(x_i)  + \Var(x_i) \cdot \Var(w_i)]  
	\only<2->{= \\  = \sum_{i=1}^{n_{in}} [\E(x_i)]^2 \cdot \Var(w_i) +  \Var(x_i) \cdot \Var(w_i)  =  \sum_{i=1}^{n_{in}} \Var(w_i) \cdot E(x_i^2)} 
	\only<3->{= \\  =  E(x^2)  \color{green} \cdot [n_{in} \cdot \Var(w)]}
	\end{multline*}

		\only<1-2>{	
	\begin{wideitemize}
			 \item  Когда нет симметрии, можно занулить только второе слагаемое
	\end{wideitemize}}
\end{frame}


\begin{frame}{Инициализация Хе}
	\begin{align*} 	
	\Var(h_i)  & =   E(x_i^2)  \cdot [n_{in} \cdot \Var(w)] \\
	x_i & = \max(0; h_{i-1}) \\ 
	\end{align*} \pause 
	
\only<2-> {Если $h_{i-1}$ симметрично распределён относительно нуля, тогда: }
	
\begin{align*} 	
\only<2->{& E(x_i^2) = \frac{1}{2} \cdot \Var(h_{i-1}) \\ }
\only<3->{& \Var(h_i) =  \frac{1}{2} \cdot \Var(h_{i-1}) \cdot [n_{in} \cdot \Var(w)]  \\ 
& \Var(w_i) = \frac{2}{n_{in}} }
\end{align*}

\vfill %
\footnotesize
{\color{blue} \url{https://arxiv.org/pdf/1502.01852.pdf}}
\end{frame}


\begin{frame}{Кратикие итоги}
\begin{wideitemize}
\item  Для симметричных функций с нулевым средним используйте инициализацию Ксавье {\color{green}  init="glorot\_uniform"} или {\color{green} }
\item  Для ReLU и им подобным инициализацию Хe {\color{green} init="he\_uniform"} или {\color{green} init="he\_nomal"}
\item  Эти две инициализации корректируют параметры распределений в зависимости от входа и выхода слоя так, чтобы поддерживать дисперсию равной единице
\end{wideitemize}
\end{frame}


\begin{frame}{Эксперимент с MNIST}
\begin{center}
	\includegraphics[width=.8\linewidth]{nicol_init.png}
\end{center}

\vfill
{\small  Источник: Николенко, страница 149}
\end{frame}


\begin{transitionframe}
	\begin{center}
		\Huge  Батч-нормализация
	\end{center}
\end{transitionframe}


\begin{frame}{Стандартизация}
\begin{center}
	\includegraphics[width=.8\linewidth]{standartization.png}
\end{center}

\begin{center}
	Какая из ситуаций лучше для SGD? 
\end{center}
\end{frame}


\begin{frame}{А что внутри?}
\begin{center}
\includegraphics[width=.7\linewidth]{distributions_1.png}
\end{center}
\end{frame}


\begin{frame}{А что внутри?}
\begin{center}
	\includegraphics[width=.7\linewidth]{distributions_2.png}
\end{center}
\end{frame}


\begin{frame}{Проблема}
\begin{wideitemize}
	\item Давайте вместо $X$ на входе использовать $\frac{X - \mu_X}{\sigma_X}$
	
	\item  Даже если мы стандартизовали вход $X$, внутри сетки может произойти несчастье и скрытый слой окажется нестандартизован 
	
	\item  Скрытые представления $h = f(XW)$  могут менять своё распределение в процессе обучения, это усложняет его  \pause
	
	\item Давайте на каждом слое вместо $h$ использовать  $\hat h = \frac{h - \mu_h}{\sigma_h}$
	
	\item На выход будем выдавать $\beta \cdot  \hat h + \gamma$, для того, чтобы у нас было больше свободы, параметры $\beta$ и $\gamma$ тоже учим 
\end{wideitemize}
\end{frame}


\begin{frame}{Batch norm (2015)}
\begin{center}
	\includegraphics[width=.7\linewidth]{distributions_nice.png}
\end{center}
\end{frame}


\begin{frame}{Batch norm (2015)}
\begin{wideitemize}
\item  Откуда взять $\mu_h$ и $\sigma_h$? 

\item Оценить по текущему батчу! 

\begin{align*} 
\mu_h =  \alpha \cdot \bar x_{batch}  + (1 - \alpha) \cdot \mu_h \\ 
\sigma_h^2 =  \alpha \cdot \hat s^2_{batch}  + (1 - \alpha) \cdot \sigma^2_h \\ 
\end{align*}

\item Коэффициенты $\beta$ и $\gamma$ оцениваются в ходе обратного распространения ошибки

\item Обучение довольно сильно ускоряется, сходимость улучшается 
\end{wideitemize}

\vfill %
\footnotesize
{\color{blue} \url{https://arxiv.org/pdf/1502.03167.pdf}}
\end{frame}


\begin{frame}{Forward pass}
\begin{center}
	\includegraphics[width=.6\linewidth]{batch_formulas.png}
\end{center}
\vfill %
\footnotesize
{\color{blue} \url{https://arxiv.org/pdf/1502.03167.pdf}}
\end{frame}


\begin{frame}{Backward pass}
\begin{center}
	\includegraphics[width=.9\linewidth]{batch_grad.png}
\end{center}
\vfill %
\footnotesize
{\color{blue} \url{https://arxiv.org/pdf/1502.03167.pdf}}
\end{frame}


\begin{frame}{Эксперимент с MNIST}
\begin{center}
	\includegraphics[width=.8\linewidth]{batch_norm.png}
\end{center}
\vfill
{\small  Источник: Николенко, страница 160}
\end{frame}


\begin{frame}{Трюки}
\begin{wideitemize}
	\item  С батч-номрализацией нужно уменьшить силу Dropout и регуляризацию 
	
	\item  Не забывайте перемешивать обучающую выборку перед каждой новой эпохой, чтобы батчи были разнообразными 
\end{wideitemize}
\end{frame}


\begin{transitionframe}
	\begin{center}
		\Huge Dropout 
	\end{center}
\end{transitionframe}



\begin{frame}{Dropout}
\begin{wideitemize}	
	\item С вероятностью $p$ отключаем нейрон 
	
	\item Делает нейроны более устойчивыми к случайным возмущениям
	
	\item Борьба с ко-адоптацией, не все соседи похожи, не все дети похожи на родителей
	
\end{wideitemize}

\begin{center}
	\includegraphics[width=.35\linewidth]{dropout.png}
\end{center}

\vfill %
\footnotesize
{\color{blue} \url{http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf}}
\end{frame}



\begin{frame}{Dropout в формулах} 
	\begin{wideitemize}
		\item 	\alert{forward pass:} 
		
		\begin{equation*}
			\begin{aligned}
			& o = f(X \cdot W + b ) \\
			\only<2->{ & \alert{o = D \cdot f(X \cdot W + b ), \quad  D = (D_1, \ldots, D_k) \sim iid \vspace{2mm} Bern(p) } }
			\end{aligned}
		\end{equation*}
		
		\only<3>{
		\vfill
		$$
		o_i = D_i \cdot f(w x_i^T + b) = \begin{cases} f(w x_i^T + b) , p \\ 0, 1-p \end{cases}
		$$
		\vfill 
		Дропаут — это просто небольшая модификация функции активации
		}
\vfill
		\only<4->{
		\item  	\alert{backward pass:} 
		\begin{equation*}
			\begin{aligned}
			& d = f'(h) \cdot W \cdot d  \\
			& \alert{ d =  D \cdot f'(h) \cdot W \cdot d} 
			\end{aligned}
		\end{equation*}			
		}	
	\end{wideitemize}
\end{frame}


\begin{frame}{Dropout}
\begin{wideitemize}
\item  При обучении мы домножаем часть выходов на $D_i$, тем самым мы изменяем только часть параметров и нейроны учатся более независимо 

\item  Dropout эквивалентен обучению $2^n$ сетей

\only<2>{\alert{ \item  Что делать на стадии тестирования?}}

 \only<3->{
\item Нам надо сымитировать работу такого ансамбля: можно отключать по очереди все возможные комбинации нейронов, получить $2^n$ прогнозов и усреднить их }

 \only<4->{ \item Но лучше просто брать по дропауту математическое ожидание 

$$
o = p \cdot f(X \cdot W + b)
$$}
\end{wideitemize}
\end{frame}


\begin{frame}{Обратный Dropout}
\begin{wideitemize}
	\item   На тесте ищем математическое ожидание: 
		$$
		o = p \cdot f(X \cdot W + b)
		$$ \pause
		
	\item \alert{Это неудобно! Надо переписывать функцию для прогнозов!}  \pause
	
	\item Давайте лучше будем домножать на $\frac{1}{p}$ на этапе обучения: 
	
	\begin{equation*}
		\begin{aligned}
			& \alert{ \text{train: }}  o = \frac{1}{p} \cdot D \cdot f(X \cdot W + b) \\ 
			& \alert{\text{test:  }}   o = f(X \cdot W + b) \\
		\end{aligned}
	\end{equation*}		
\end{wideitemize}
\end{frame}

%В стандартной нейронной сети производная, полученная каждым параметром, сообщает ему, как он должен измениться, чтобы, учитывая деятельность остальных блоков, минимизировать функцию конечных потерь. Поэтому блоки могут меняться, исправляя при этом ошибки других блоковкарпатый dropout. Это может привести к чрезмерной совместной адаптации (co-adaptation), что, в свою очередь, приводит к переобучению, поскольку эти совместные адаптации невозможно обобщить на данные, не участвовавшие в обучении. Мы выдвигаем гипотезу, что Dropout предотвращает совместную адаптацию для каждого скрытого блока, делая присутствие других скрытых блоков ненадежным. Поэтому скрытый блок не может полагаться на другие блоки в исправлении собственных ошибок.

%

%Dropout хорошо работает на практике, потому что предотвращает взаимоадаптацию нейронов на этапе обучения.


\begin{transitionframe}
	\begin{center}
		\Huge  Другие эвристики для обучения сеток 
	\end{center}
\end{transitionframe}


\begin{frame}{Предобучение}
\begin{wideitemize}
	\item  Обучаем каждый нейрон на рандомной подвыборке, каждый нейрон впитает какие-то отдельные её особенности, после скрепляем все нейроны вместе и продолжаем обучение на всей выборке
	
	\item  \alert{На будущее:} обучаем на корпусе картинок автокодировщик, encoder благодаря этому учится выделять наиболее важные фичи, которые позволяют эффективно сжимать изображения. После срезаем decoder и на его месте достраиваем слои для решения нашей задаче, запускаем обычное дообучение.
\end{wideitemize}
\end{frame}


\begin{frame}{Динамическое наращивание сети}
\begin{wideitemize}
\item  Обучение сети при заведомо недостаточном числе нейронов $H$
\item После стабилизации функции потерь — добавление нового нейрона и его инициализация путём обучения 
\begin{itemize}
	\item либо по случайной подвыборке 
	\item либо по объектам с наибольшими значениями потерь 
	\item либо по случайному подмножеству входов
	\item либо из различных случайных начальных приближений
\end{itemize}
\item Снова итерации BackProp
\end{wideitemize}

\vfill
\begin{center}
\alert{Эмпирический опыт:} Общее время обучения обычно лишь в $1.5-2$ раза больше, чем если бы в сети сразу было итоговое число нейронов. Полезная информация, накопленная сетью не теряется при добавлении нейронов.
\end{center}
\end{frame}


\begin{frame}{Прореживание сети}
\begin{wideitemize}
\item Начать с большого количество нейронов и удалять незначимые по какому-нибудь критерию 
\item \alert{Пример:} обнуляем вес, смотрим как сильно упала ошибка, сортируем все вязи по этому критерию, удаляем $N$ наименее значимых
\item После прореживания снова запускаем backprop
\item Если качество модели сильно упала, надо вернуть последние удалённые связи 
\end{wideitemize}
\end{frame}


\begin{transitionframe}
	\begin{center}
		\Huge  Другие хаки
	\end{center}
\end{transitionframe}

\begin{frame}{Уже обсуждали}
\begin{wideitemize}
	\item $l_1$ и $l_2$ регуляризация 
	\item Ранняя остановка 
	\item Различные новые градиентные спуски, ускоряющие процедуру сходимости 
\end{wideitemize}
\end{frame}

\begin{frame}{Ещё обсудим}
\begin{wideitemize}
	\item Скип-конекшены 
	\item Аугментация данных 
	\item Более забубуенистые архитектуры 
\end{wideitemize}
\end{frame}
\end{document}


%%%%%%%  В презу с эвристиками ! 

%{
%	\usebackgroundtemplate{ 
%		\includegraphics[width=\paperwidth]{overfitting.png}}
%	\begin{frame}
%\end{frame}
%}
%
%
%\begin{frame}{Early stopping}
%\begin{center}
%\includegraphics[width=0.5\paperwidth]{early_stopping.png}
%\end{center}
%\begin{wideitemize}
%\item Будем останавливать обучение, когда качество на валидации начинает падать
%\end{wideitemize}
%\end{frame}
%
%
%\begin{frame}{Регуляризация}
%\begin{wideitemize}
%\item $L_2$: приплюсовываем к функции потерь $\lambda \cdot \sum w_i^2$
%
%\item $L_1$: приплюсовываем к функции потерь $\lambda \cdot \sum |w_i|$
%
%\item Можно регуляризовать не всю сетку, а отдельный нейрон или слой 
%
%\item Не даёт нейрону сфокусироваться на слишком выделяющемся входе	
%\end{wideitemize}
%\end{frame}
%
%
%\begin{frame}{Регуляризация}
%\begin{wideitemize}
%\item В keras можно добавить для каждого слоя на три вида связей: 
%
%\item \textbf{kernel\_regulirizer} - на матрицу весов слоя;
%
%\item \textbf{bias\_regulirizer} - на вектор свободных членов;
%
%\item \textbf{kernel\_regulirizer} - на вектор выходов.
%
%\item \textbf{Делается примерно так:}
%
%\mbox{ }
%
%\pgr{\hspace{50pt}} model.add(Dense(256, inpit\_dim = 32, \\
%\pgr{\hspace{80pt}}  kernel\_regulirizer = regulizers.l1(0.001), \\
%\pgr{\hspace{80pt}} bias\_regulirizer = regulizers.l2(0.1), \\
%\pgr{\hspace{80pt}} activity\_regulirizer = regulizers.l2(0.01))) 
%\end{wideitemize}
%\end{frame}
%
%
%\begin{frame}{Dropout}
%\begin{center}
%\includegraphics[width=0.5\paperwidth]{dropout2.png}
%\end{center}
%\begin{wideitemize}
%\item Предотвращает коадоптацию нейронов и делает их более устойчивыми к случайным возмущениям 
%
%\item На самом деле эквивалентен обучению $2^n$ моделей 
%
%\end{wideitemize}
%\end{frame}
%
%
%\begin{frame}{Взаимосвязи}
%\begin{wideitemize}
%\item  На практике обычно используют Dropout. Действия всех этих регуляризаторов оказывается схожим: 
%
%\item Например, в [1] написано: \\ \mbox{   } \\
%
%\texttt{«We show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix»}
%
%\item  У Гудфеллоу в Глубоком обучении на стр. 218 можно найти, что рання остановка для линейных моделей эквивалентна $l_2$ регуляризации с MSE, обучаемой SGD.
%
%\end{wideitemize}
%
%\vfill %
%\footnotesize 
%\color{blue} [1] \url{https://arxiv.org/abs/1307.1493} 
%\end{frame}
%
%
%\begin{frame}{Размеры сеток и переобучение}
%\begin{center}
%\includegraphics[width=0.7\paperwidth]{big_base_small_fit.png}
%\end{center}
%\end{frame}
